# Do not edit this file.
import os
import torch
import random
import numpy as np
from os.path import join
from pprint import pprint

from torch.utils.data import DataLoader
from metaflow import FlowSpec, step, Parameter
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint

from fashion.system import ProductionDataset, FashionClassifierSystem
from fashion.utils import load_config
from fashion.paths import LOG_DIR, CONFIG_DIR, CHECKPOINT_DIR, DATA_DIR


class FinetuneFlow(FlowSpec):
  r"""A MetaFlow that finetunes an image classifier on new images of fashion clothing.
  Does not do any evaluation.

  Arguments
  ---------
  config (str, default: ./configs/train.json): path to a configuration file
  checkpoint (str, default: ./checkpoints/model.ckpt): path to a trained checkpoint file
  dataset (str, default: ./datasets/production/random.pt): path to a dataset for finetuning
  augment (bool, default: False): whether to augment the training dataset or not
  """
  config_path = Parameter('config', help = 'path to config file', default = join(CONFIG_DIR, 'finetune.json'))
  checkpoint_path = Parameter('checkpoint', help = 'path to checkpoint file', default = join(CHECKPOINT_DIR, 'model.ckpt'))
  dataset_path = Parameter('dataset', help = 'path to dataset file', default = join(DATA_DIR, 'production/random.pt'))
  augment = Parameter('augment', help = 'augment training data', default = False)

  @step
  def start(self):
    r"""Start node.
    Set random seeds for reproducibility.
    """
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)

    self.next(self.init_system)

  @step
  def init_system(self):
    r"""Instantiates a data module, pytorch lightning module, 
    and lightning trainer instance.
    """
    # configuration files contain all hyperparameters
    config = load_config(self.config_path)

    # a callback to save best model weights
    checkpoint_callback = ModelCheckpoint(
      dirpath = config.save_dir,
      monitor = 'train_acc',
      mode = 'max',
      save_top_k = 1,  # save top 1 checkpoints
      verbose = True,
    )

    trainer = Trainer(
      max_epochs = config.optimizer.max_epochs,
      resume_from_checkpoint=self.checkpoint_path,
      callbacks = [checkpoint_callback],
    )

    # when we save these objects to a `step`, they will be available
    # for use in the next step, through not steps after.
    self.trainer = trainer
    self.config = config

    self.next(self.finetune_model)

  @step
  def finetune_model(self):
    """Calls `fit` on the trainer."""

    # a data module wraps around training, dev, and test datasets
    ds = ProductionDataset(self.dataset_path, return_hidden_labels = True)
    dl = DataLoader(ds, batch_size = 32, shuffle = True)

    # Loading the checkpoint file & initialize model
    system = FashionClassifierSystem(self.config)

    # Call `fit` on the trainer with `system` and `dl`.
    self.trainer.fit(system, train_dataloaders=dl, ckpt_path=self.checkpoint_path)

    self.next(self.end)

  @step
  def end(self):
    """End node!"""
    print('done! great work!')


if __name__ == "__main__":
  """
  To validate this flow, run `python finetune.py`. To list
  this flow, run `python finetune.py show`. To execute
  this flow, run `python finetune.py run`.

  You may get PyLint errors from `numpy.random`. If so,
  try adding the flag:

    `python finetune.py --no-pylint run`

  If you face a bug and the flow fails, you can continue
  the flow at the point of failure:

    `python finetune.py resume`
  
  You can specify a run id as well.
  """
  flow = FinetuneFlow()
